# -*- coding: utf-8 -*-
"""ML lab (6/8).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vfLfk4XJi8lP5h1yY3FqeDJG-dBJS5zP
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

colnames=['Area','Rooms','Price']
dataset=pd.read_csv("https://raw.githubusercontent.com/nishithkotak/machine-learning/refs/heads/master/ex1data2.txt",
                    names=colnames)

dataset

dataset.describe()

Area=dataset.iloc[0:dataset.shape[0],0:1]
Rooms=dataset.iloc[0:dataset.shape[0],1:2]
Price=dataset.iloc[0:dataset.shape[0],2:3]

dataset.shape

#function of normalization
def featureNormalize(x):
    mean=np.mean(x,axis=0)
    std=np.std(x,axis=0)
    X_normalized=(x-mean)/std
    return X_normalized,mean,std

data_norm=dataset.values
m=data_norm.shape[0]
#take the feature vectors
x2=data_norm[:,0:2].reshape(m,2)
x2_norm,mean,std=featureNormalize(x2)
y2=data_norm[:,2:3].reshape(m,1) #price column

x2_norm

theta_array=np.zeros((3,1))

def Hypothesis(theta_array , x1 , x2) :
  return theta_array[0] + theta_array[1]*x1 + theta_array[2]*x2

def Cost_Function(theta_array,x1,x2,y,m):
  total_cost = 0
  for i in range(m):
    total_cost += (Hypothesis(theta_array,x1[i] , x2[i]) - y[i])**2
    return total_cost/(2*m)

def Gradient_Descent(theta_array , x1, x2, y , m ,alpha) :
  summation_0 = 0
  summation_1 = 0
  summation_2 = 0
  for i in range(m):
    summation_0 += (Hypothesis(theta_array,x1[i] , x2[i]) - y[i])
    summation_1 += ((Hypothesis(theta_array,x1[i] , x2[i]) - y[i])*x1[i])
    summation_2 += ((Hypothesis(theta_array,x1[i] , x2[i]) - y[i])*x2[i])
    new_theta0 = theta_array[0] - (alpha/m)*summation_0
    new_theta1 = theta_array[1] - (alpha/m)*summation_1
    new_theta2 = theta_array[2] - (alpha/m)*summation_2
    new_theta = [new_theta0 , new_theta1 , new_theta2]
    return new_theta

def Training(x1, x2, y, alpha, iters):
  theta_0 = 0
  theta_1 = 0
  theta_2 = 0
  theta_array = [theta_0, theta_1 ,theta_2]
  m = len(x1)
  cost_values = []
  for i in range(iters):
    theta_array = Gradient_Descent(theta_array, x1 ,x2, y, m, alpha)
    loss = Cost_Function(theta_array, x1 ,x2, y, m)
    cost_values.append(loss)
    y_new = theta_array[0] + theta_array[1]*x1 + theta_array[2]*x2
    return theta_array , cost_values

alpha=0.001
iters=500

cost,theta=Training(x2_norm[:,0:1],x2_norm[:,1:2],y2,alpha,iters)

x_axis = np.arange(0, len(cost), step=1)
plt.plot(x_axis, cost)
plt.xlabel("Iterations")
plt.ylabel("Cost Values")
plt.title("Loss Graph")
plt.show()
print(theta)

